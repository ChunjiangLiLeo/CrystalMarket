{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e6f71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 990 comments from r/Crystals and saved to 'reddit_comments.csv'\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "##\n",
    "##\n",
    "##\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Reddit API with new credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"",
    "    client_secret=\"\",\n",
    "    user_agent=\"",\n",
    "    redirect_uri=\""\n",
    ")\n",
    "\n",
    "# Select a subreddit\n",
    "subreddit_name = \"Crystals\"  # Change this to any subreddit you want to scrape\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "# Fetch latest comments\n",
    "comments = []\n",
    "for comment in subreddit.comments(limit=4000):  # Adjust limit as needed\n",
    "    comments.append(comment.body)\n",
    "\n",
    "# Save comments to CSV\n",
    "df = pd.DataFrame(comments, columns=[\"comment\"])\n",
    "df.to_csv(\"reddit_comments.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Scraped {len(comments)} comments from r/{subreddit_name} and saved to 'reddit_comments.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4484857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting comment scraping for r/Crystals\n",
      "✅ Scraped 100/200000 comments so far...\n",
      "✅ Scraped 200/200000 comments so far...\n",
      "✅ Scraped 300/200000 comments so far...\n",
      "✅ Scraped 400/200000 comments so far...\n",
      "✅ Scraped 500/200000 comments so far...\n",
      "✅ Scraped 600/200000 comments so far...\n",
      "✅ Scraped 700/200000 comments so far...\n",
      "✅ Scraped 800/200000 comments so far...\n",
      "✅ Scraped 900/200000 comments so far...\n",
      "✅ Scraped 1000/200000 comments so far...\n",
      "✅ Scraped 1100/200000 comments so far...\n",
      "✅ Scraped 1200/200000 comments so far...\n",
      "✅ Scraped 1300/200000 comments so far...\n",
      "✅ Scraped 1400/200000 comments so far...\n",
      "✅ Scraped 1500/200000 comments so far...\n",
      "✅ Scraped 1600/200000 comments so far...\n",
      "✅ Scraped 1700/200000 comments so far...\n",
      "✅ Scraped 1800/200000 comments so far...\n",
      "✅ Scraped 1900/200000 comments so far...\n",
      "✅ Scraped 2000/200000 comments so far...\n",
      "✅ Scraped 2100/200000 comments so far...\n",
      "✅ Scraped 2200/200000 comments so far...\n",
      "✅ Scraped 2300/200000 comments so far...\n",
      "✅ Scraped 2400/200000 comments so far...\n",
      "✅ Scraped 2500/200000 comments so far...\n",
      "✅ Scraped 2600/200000 comments so far...\n",
      "✅ Scraped 2700/200000 comments so far...\n",
      "✅ Scraped 2800/200000 comments so far...\n",
      "✅ Scraped 2900/200000 comments so far...\n",
      "✅ Scraped 3000/200000 comments so far...\n",
      "✅ Scraped 3100/200000 comments so far...\n",
      "✅ Scraped 3200/200000 comments so far...\n",
      "✅ Scraped 3300/200000 comments so far...\n",
      "✅ Scraped 3400/200000 comments so far...\n",
      "✅ Scraped 3500/200000 comments so far...\n",
      "✅ Scraped 3600/200000 comments so far...\n",
      "✅ Scraped 3700/200000 comments so far...\n",
      "✅ Scraped 3800/200000 comments so far...\n",
      "✅ Scraped 3900/200000 comments so far...\n",
      "✅ Scraped 4000/200000 comments so far...\n",
      "✅ Scraped 4100/200000 comments so far...\n",
      "✅ Scraped 4200/200000 comments so far...\n",
      "✅ Scraped 4300/200000 comments so far...\n",
      "✅ Scraped 4400/200000 comments so far...\n",
      "✅ Scraped 4500/200000 comments so far...\n",
      "✅ Scraped 4600/200000 comments so far...\n",
      "✅ Scraped 4700/200000 comments so far...\n",
      "✅ Scraped 4800/200000 comments so far...\n",
      "✅ Scraped 4900/200000 comments so far...\n",
      "✅ Scraped 5000/200000 comments so far...\n",
      "✅ Scraped 5100/200000 comments so far...\n",
      "✅ Scraped 5200/200000 comments so far...\n",
      "✅ Scraped 5300/200000 comments so far...\n",
      "✅ Scraped 5400/200000 comments so far...\n",
      "✅ Scraped 5500/200000 comments so far...\n",
      "✅ Scraped 5600/200000 comments so far...\n",
      "✅ Scraped 5700/200000 comments so far...\n",
      "✅ Scraped 5800/200000 comments so far...\n",
      "✅ Scraped 5900/200000 comments so far...\n",
      "✅ Scraped 6000/200000 comments so far...\n",
      "✅ Scraped 6100/200000 comments so far...\n",
      "✅ Scraped 6200/200000 comments so far...\n",
      "✅ Scraped 6300/200000 comments so far...\n",
      "✅ Scraped 6400/200000 comments so far...\n",
      "✅ Scraped 6500/200000 comments so far...\n",
      "✅ Scraped 6600/200000 comments so far...\n",
      "✅ Scraped 6700/200000 comments so far...\n",
      "✅ Scraped 6800/200000 comments so far...\n",
      "✅ Scraped 6900/200000 comments so far...\n",
      "✅ Scraped 7000/200000 comments so far...\n",
      "✅ Scraped 7100/200000 comments so far...\n",
      "✅ Scraped 7200/200000 comments so far...\n",
      "✅ Scraped 7300/200000 comments so far...\n",
      "✅ Scraped 7400/200000 comments so far...\n",
      "✅ Scraped 7500/200000 comments so far...\n",
      "✅ Scraped 7600/200000 comments so far...\n",
      "✅ Scraped 7700/200000 comments so far...\n",
      "✅ Scraped 7800/200000 comments so far...\n",
      "✅ Scraped 7900/200000 comments so far...\n",
      "✅ Scraped 8000/200000 comments so far...\n",
      "✅ Scraped 8100/200000 comments so far...\n",
      "✅ Scraped 8200/200000 comments so far...\n",
      "✅ Scraped 8300/200000 comments so far...\n",
      "✅ Scraped 8400/200000 comments so far...\n",
      "✅ Scraped 8500/200000 comments so far...\n",
      "✅ Scraped 8600/200000 comments so far...\n",
      "✅ Scraped 8700/200000 comments so far...\n",
      "✅ Scraped 8800/200000 comments so far...\n",
      "✅ Scraped 8900/200000 comments so far...\n",
      "✅ Scraped 9000/200000 comments so far...\n",
      "✅ Scraped 9100/200000 comments so far...\n",
      "✅ Scraped 9200/200000 comments so far...\n",
      "✅ Scraped 9300/200000 comments so far...\n",
      "✅ Scraped 9400/200000 comments so far...\n",
      "✅ Scraped 9500/200000 comments so far...\n",
      "✅ Scraped 9600/200000 comments so far...\n",
      "✅ Scraped 9700/200000 comments so far...\n",
      "✅ Scraped 9800/200000 comments so far...\n",
      "✅ Scraped 9900/200000 comments so far...\n",
      "✅ Scraped 10000/200000 comments so far...\n",
      "✅ Scraped 10100/200000 comments so far...\n",
      "✅ Scraped 10200/200000 comments so far...\n",
      "✅ Scraped 10300/200000 comments so far...\n",
      "✅ Scraped 10400/200000 comments so far...\n",
      "✅ Scraped 10500/200000 comments so far...\n",
      "✅ Scraped 10600/200000 comments so far...\n",
      "✅ Scraped 10700/200000 comments so far...\n",
      "✅ Scraped 10800/200000 comments so far...\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Reddit API with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",\n",
    "    client_secret=\"\",\n",
    "    user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
    "    redirect_uri=\"http://localhost:8080/\"\n",
    ")\n",
    "\n",
    "# Select a subreddit\n",
    "subreddit_name = \"Crystals\"  # Change this to any subreddit\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "# Fetch comments with pagination\n",
    "comments = []\n",
    "comment_count = 0\n",
    "limit = 200000  # Target number of comments\n",
    "batch_size = 100  # Number of comments per request\n",
    "iteration = 0\n",
    "\n",
    "print(f\"🔄 Starting comment scraping for r/{subreddit_name}\")\n",
    "\n",
    "while comment_count < limit:\n",
    "    try:\n",
    "        new_comments = list(subreddit.comments(limit=batch_size))\n",
    "        if not new_comments:\n",
    "            print(\"🚀 No more comments available!\")\n",
    "            break  # Stop if there are no more comments\n",
    "        \n",
    "        comments.extend([comment.body for comment in new_comments])\n",
    "        comment_count += len(new_comments)\n",
    "        iteration += 1\n",
    "\n",
    "        print(f\"✅ Scraped {comment_count}/{limit} comments so far...\")\n",
    "\n",
    "        time.sleep(2)  # Avoid rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching comments: {e}\")\n",
    "        break  # Stop loop on error\n",
    "\n",
    "# Save comments to CSV\n",
    "df = pd.DataFrame(comments, columns=[\"comment\"])\n",
    "df.to_csv(\"crystal_comments.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Scraped {comment_count} comments from r/{subreddit_name} and saved to 'reddit_comments.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a35f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
